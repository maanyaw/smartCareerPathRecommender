# -*- coding: utf-8 -*-
"""SmartCareerPathRecommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Eqv8q0Hgv8jjSP28ETbHZvwfJuFgMf20
"""

#Load & Explore + Sample Parsed Resumes
import pandas as pd

# STEP 1: Load the CSV files
people_df = pd.read_csv('01_people.csv')
abilities_df = pd.read_csv('02_abilities.csv')
education_df = pd.read_csv('03_education.csv')
experience_df = pd.read_csv('04_experience.csv')
person_skills_df = pd.read_csv('05_person_skills.csv')
skills_df = pd.read_csv('06_skills.csv')

# Show initial structure
print("People:", people_df.columns)
print("Education:", education_df.columns)
print("Experience:", experience_df.columns)
print("Person-Skills:", person_skills_df.columns)
print("Skills:", skills_df.columns)

#Merge and Create Cleaned Skill List
# Merge person_skills with skills to get skill names
person_skills_df = person_skills_df.merge(skills_df, how='left', on='skill')

# Normalize skills: lowercase, strip whitespace, drop duplicates
person_skills_df['skill'] = person_skills_df['skill'].str.lower().str.strip()
person_skills_df = person_skills_df.drop_duplicates()

#Create Sample Parsed Resume(s)
# Select a few person IDs (first 3)
sample_ids = people_df['person_id'].head(3).tolist()
parsed_resumes = []

for pid in sample_ids:
    person = people_df[people_df['person_id'] == pid].iloc[0]

    # Skills
    skills = person_skills_df[person_skills_df['person_id'] == pid]['skill'].tolist()

    # Education
    edu = education_df[education_df['person_id'] == pid][['institution', 'program', 'start_date', 'location']].to_dict(orient='records')

    # Experience
    exp = experience_df[experience_df['person_id'] == pid][['firm', 'title', 'start_date', 'end_date', 'location']].to_dict(orient='records')

    # Abilities (if needed)
    abilities = abilities_df[abilities_df['person_id'] == pid]['ability'].tolist() if 'ability' in abilities_df.columns else []

    resume_dict = {
        'name': person['name'],
        'email': person.get('email', ''),
        'skills': skills,
        'education': edu,
        'experience': exp,
        'abilities': abilities
    }

    parsed_resumes.append(resume_dict)

# View parsed resume(s)
import json
print(json.dumps(parsed_resumes, indent=2))

import pandas as pd

# STEP 1: Load the Job Roles dataset
# It looks like there's a UnicodeDecodeError, which usually happens when the file is saved with an encoding other than the default (UTF-8).
# We will try reading the file using a different encoding, such as 'latin-1'.
try:
    jd_df = pd.read_csv("/content/IT_Job_Roles_Skills.csv", encoding='utf-8')
except UnicodeDecodeError:
    jd_df = pd.read_csv("/content/IT_Job_Roles_Skills.csv", encoding='latin-1')


# STEP 2: View structure and columns
print("Columns available:", jd_df.columns)
print(jd_df.head())

#STEP 3: Clean and Normalize Skills
# Rename columns if needed
jd_df.rename(columns=lambda x: x.strip().lower().replace(' ', '_'), inplace=True)

# Check exact skill column name (adjust if different)
skill_col = 'skills'  # change if your dataset uses something else
role_col = 'job_title'  # or similar

# Drop any missing skill rows
jd_df = jd_df.dropna(subset=[skill_col, role_col])

# Normalize and tokenize skill lists
jd_df[skill_col] = jd_df[skill_col].apply(lambda x: [skill.strip().lower() for skill in x.split(',')])

# Optional: limit to top 15 job roles for faster prototyping
sample_jds = jd_df[[role_col, skill_col]].head(15).reset_index(drop=True)

# Display sample
for index, row in sample_jds.iterrows():
    print(f"\nüîπ Role: {row[role_col]}")
    print(f"   ‚û§ Skills: {row[skill_col]}")

#Resume-to-Job Role Matching (Jaccard Similarity)
# Function to calculate Jaccard similarity between two lists
def jaccard_similarity(list1, list2):
    set1, set2 = set(list1), set(list2)
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    return intersection / union if union != 0 else 0

# Assuming you have 'parsed_resumes' from earlier
for resume in parsed_resumes:
    print(f"\nüìÑ Resume: {resume['name']}")
    resume_skills = [s.strip().lower() for s in resume['skills']]

    role_scores = []
    for _, row in sample_jds.iterrows():
        role = row[role_col]
        skills = row[skill_col]
        score = jaccard_similarity(resume_skills, skills)
        role_scores.append((role, score))

    # Sort roles by similarity score
    top_roles = sorted(role_scores, key=lambda x: x[1], reverse=True)[:3]

    # Display top 3 matched roles
    print("üéØ Top Job Role Matches:")
    for role, score in top_roles:
        print(f" - {role} ({round(score * 100, 2)}% match)")

# Re-run this if needed to ensure resume_skills and sample_jds are loaded
def skill_gap_analysis(resume_skills, required_skills):
    resume_set = set([s.lower().strip() for s in resume_skills])
    required_set = set([s.lower().strip() for s in required_skills])

    matched = sorted(list(resume_set.intersection(required_set)))
    missing = sorted(list(required_set.difference(resume_set)))

    return matched, missing

# Loop through each resume and its top 3 matches
for resume in parsed_resumes:
    print(f"\nüìÑ Resume: {resume['name']}")
    resume_skills = [s.lower().strip() for s in resume['skills']]

    role_scores = []
    for _, row in sample_jds.iterrows():
        role = row[role_col]
        required_skills = row[skill_col]
        score = jaccard_similarity(resume_skills, required_skills)
        role_scores.append((role, score, required_skills))

    # Top 3 roles
    top_roles = sorted(role_scores, key=lambda x: x[1], reverse=True)[:3]

    for role, score, required_skills in top_roles:
        matched, missing = skill_gap_analysis(resume_skills, required_skills)

        print(f"\nüîπ Role: {role} ({round(score * 100, 2)}% match)")
        print(f"‚úÖ Matched Skills ({len(matched)}): {matched}")
        print(f"‚ùå Missing Skills ({len(missing)}): {missing}")

# Step 1: Create Dummy Course Dataset (in-memory)
# Sample course catalog (you can expand or load from CSV later)
course_catalog = [
    {'skill': 'python', 'course_name': 'Python for Everybody', 'provider': 'Coursera', 'link': 'https://coursera.org/learn/python'},
    {'skill': 'sql', 'course_name': 'SQL for Data Science', 'provider': 'Coursera', 'link': 'https://coursera.org/learn/sql-data-science'},
    {'skill': 'power bi', 'course_name': 'Power BI Essentials', 'provider': 'Udemy', 'link': 'https://udemy.com/course/power-bi-essentials/'},
    {'skill': 'excel', 'course_name': 'Excel Skills for Business', 'provider': 'Coursera', 'link': 'https://coursera.org/learn/excel'},
    {'skill': 'tableau', 'course_name': 'Tableau A-Z', 'provider': 'Udemy', 'link': 'https://udemy.com/course/tableau-data-visualization/'},
    {'skill': 'statistics', 'course_name': 'Intro to Statistics', 'provider': 'edX', 'link': 'https://edx.org/course/statistics'}
]

#Step 2: Map Missing Skills to Courses
# Convert catalog to DataFrame for easy lookup
course_df = pd.DataFrame(course_catalog)

# Updated Skill Gap Loop with Course Recommendation
for resume in parsed_resumes:
    print(f"\nüìÑ Resume: {resume['name']}")
    resume_skills = [s.lower().strip() for s in resume['skills']]

    role_scores = []
    for _, row in sample_jds.iterrows():
        role = row[role_col]
        required_skills = row[skill_col]
        score = jaccard_similarity(resume_skills, required_skills)
        role_scores.append((role, score, required_skills))

    top_roles = sorted(role_scores, key=lambda x: x[1], reverse=True)[:3]

    for role, score, required_skills in top_roles:
        matched, missing = skill_gap_analysis(resume_skills, required_skills)

        print(f"\nüîπ Role: {role} ({round(score * 100, 2)}% match)")
        print(f"‚úÖ Matched Skills: {matched}")
        print(f"‚ùå Missing Skills: {missing}")

        # Recommend courses for missing skills
        print("üéì Course Recommendations:")
        for skill in missing:
            course = course_df[course_df['skill'] == skill]
            if not course.empty:
                for _, row in course.iterrows():
                    print(f" - {skill.title()}: {row['course_name']} ({row['provider']}) ‚Üí {row['link']}")
            else:
                print(f" - {skill.title()}: No course found.")

#Step 1: Unzip and Load the Dataset
import zipfile
import os

# Unzip
with zipfile.ZipFile('/content/archive (2).zip', 'r') as zip_ref:
    zip_ref.extractall('/courses_json')

# List files to check structure
files = os.listdir('/data/courses_json')
print("Files in dataset:", files[:5])  # Show first 5

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import matplotlib.pyplot as plt
# import re
# from io import StringIO
# import fitz  # PyMuPDF
# 
# # ---------- Page Config ----------
# st.set_page_config(page_title="Career Path Recommender", layout="centered")
# 
# # ---------- Custom Styles ----------
# st.markdown("""
# <style>
#     .block-container { padding-top: 2rem; padding-bottom: 2rem; }
#     .stButton>button { background-color: #4CAF50; color: white; }
#     .stDownloadButton>button { background-color: #0099ff; color: white; }
# </style>
# """, unsafe_allow_html=True)
# 
# # ---------- Title ----------
# st.title("üß† Smart Career Path Recommender")
# 
# # ---------- PDF Upload ----------
# st.subheader("üìÑ Upload your Resume (PDF)")
# uploaded_file = st.file_uploader("Choose your resume (PDF format only)", type="pdf")
# 
# resume_text = ""
# 
# # Load job and course datasets
# try:
#     job_df = pd.read_csv("/content/IT_Job_Roles_Skills.csv", encoding='utf-8')
# except UnicodeDecodeError:
#     job_df = pd.read_csv("/content/IT_Job_Roles_Skills.csv", encoding='latin-1')
# 
# job_df.rename(columns=lambda x: x.strip().lower().replace(' ', '_'), inplace=True)
# job_df = job_df.dropna(subset=['skills', 'job_title'])
# job_df['skills'] = job_df['skills'].apply(lambda x: [skill.strip().lower() for skill in x.split(',')])
# 
# course_df = pd.read_csv("/content/courses_final.csv")
# 
# # ---------- Extract Resume Text ----------
# if uploaded_file:
#     pdf_reader = fitz.open(stream=uploaded_file.read(), filetype="pdf")
#     for page in pdf_reader:
#         resume_text += page.get_text()
#     st.success("‚úÖ Resume uploaded and text extracted.")
# else:
#     st.warning("Please upload a resume to proceed.")
# 
# # ---------- Main Logic ----------
# if uploaded_file and st.button("Analyze"):
# 
#     def extract_skills(text):
#         text = text.lower()
#         text = re.sub(r'[^a-zA-Z\s]', '', text)
#         words = set(text.split())
# 
#         known_skills = set()
#         for skills_list in job_df['skills']:
#             if isinstance(skills_list, list):
#                 known_skills.update(skills_list)
# 
#         # Synonym mapping
#         synonym_map = {
#             'github': 'git',
#             'rest': 'rest apis',
#             'restapi': 'rest apis',
#             'dsa': 'data structures',
#             'cloud': 'cloud computing',
#             'api': 'rest apis',
#         }
# 
#         matched = set()
#         for word in words:
#             if word in known_skills:
#                 matched.add(word)
#             elif word in synonym_map and synonym_map[word] in known_skills:
#                 matched.add(synonym_map[word])
# 
#         return list(matched)
# 
#     resume_skills = extract_skills(resume_text)
# 
#     if not resume_skills:
#         st.warning("No recognizable skills found in the resume.")
#     else:
#         st.subheader("‚úÖ Extracted Resume Skills")
#         st.write(", ".join(resume_skills))
# 
#         # --- Match Top Roles ---
#         def score_match(row):
#             required = row['skills']
#             matched = list(set(required).intersection(resume_skills))
#             return len(matched), matched
# 
#         job_df['match_score'], job_df['matched_skills'] = zip(*job_df.apply(score_match, axis=1))
#         top_roles = job_df.sort_values(by='match_score', ascending=False).head(3)
# 
#         # --- Display Matched Roles ---
#         st.subheader("üß≠ Best Matched Job Roles")
#         report = StringIO()
#         report.write("Resume Skills:\n")
#         report.write(", ".join(resume_skills) + "\n\n")
# 
#         for _, row in top_roles.iterrows():
#             required = row['skills']
#             matched = row['matched_skills']
#             missing = list(set(required) - set(matched))
# 
#             st.markdown(f"### üîπ {row['job_title']}")
#             st.markdown(f"‚úîÔ∏è **Matched Skills**: {', '.join(matched)}")
#             st.markdown(f"‚ùå **Missing Skills**: {', '.join(missing) if missing else 'None'}")
# 
#             # Pie Chart
#             labels = [f'Matched ({len(matched)})', f'Missing ({len(missing)})']
#             sizes = [len(matched), len(missing)]
#             fig, ax = plt.subplots()
#             ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
#             ax.axis('equal')
#             st.pyplot(fig)
# 
#             # Course Recommendations
#             st.markdown("üéì **Recommended Courses:**")
#             for skill in missing:
#                 pattern = re.compile(re.escape(skill), re.IGNORECASE)
#                 matched_courses = course_df[course_df['name'].apply(lambda x: bool(pattern.search(str(x))))]
# 
#                 if not matched_courses.empty:
#                     for _, course in matched_courses.head(2).iterrows():
#                         name = course.get('name', 'Unnamed Course')
#                         url = course.get('url', '#')
#                         st.markdown(f"- {skill.title()}: [{name}]({url})")
#                         report.write(f"- {skill.title()}: {name} ({url})\n")
#                 else:
#                     st.markdown(f"- {skill.title()}: No course found.")
#                     report.write(f"- {skill.title()}: No course found.\n")
# 
#             report.write(f"\nRole: {row['job_title']}\n")
#             report.write(f"Matched Skills: {', '.join(matched)}\n")
#             report.write(f"Missing Skills: {', '.join(missing)}\n\n")
# 
#         # --- Download Report ---
#         st.download_button(
#             label="üì• Download Report",
#             data=report.getvalue(),
#             file_name="career_recommendation.txt",
#             mime="text/plain"
#         )
#

#Step 2: import json

course_data = []

for file in files:
    if file.endswith('.json'):
        try:
            with open(f"/courses_json/{file}", 'r', encoding='utf-8') as f:
                raw = f.read().strip()
                if raw.startswith('['):
                    parsed = json.loads(raw)
                    course_data.extend(parsed)  # list of courses
                else:
                    parsed = json.loads(raw)
                    course_data.append(parsed)  # single course
        except Exception as e:
            print(f"‚ùå Could not parse {file}: {e}")

# Convert to DataFrame if any valid data found
if course_data:
    course_df = pd.DataFrame(course_data)
    print("‚úÖ Loaded courses:", len(course_df))
    print(course_df.head())
else:
    print("‚ö†Ô∏è Still no valid courses found.")

course_df.columns.tolist()

course_df[['title', 'external_url']].dropna().to_csv("/content/courses_final.csv", index=False)

course_df = pd.read_csv("/content/courses_final.csv")

course_df.columns = ['name', 'url']

# Step 3: Inspect and Clean Course Data
# Inspect available columns
print("üîé Columns available:", course_df.columns.tolist())
print("\nüì¶ Sample Row:")
print(course_df.iloc[0])

# Normalize column names
course_df.columns = [c.lower().strip().replace(' ', '_') for c in course_df.columns]

# Check columns after renaming
print(course_df.columns.tolist())

# Safely proceed only if 'skills' is a valid column
if 'skills' in course_df.columns:
    course_df = course_df.dropna(subset=['skills'])
    course_df['skills'] = course_df['skills'].astype(str).str.lower().str.strip()

# Normalize organization/platform if it exists
if 'organization' in course_df.columns:
    course_df['platform'] = course_df['organization'].astype(str).str.strip().str.replace('[', '', regex=False).str.replace(']', '', regex=False).str.replace("'", '', regex=False)
else:
    course_df['platform'] = 'Unknown'

# Fill missing values in name and url based on available columns
course_df.fillna({
    'title': 'Untitled',
    'external_url': 'No link',
    'platform': 'Unknown'
}, inplace=True)

course_df.columns.tolist()

# Normalize column names (already done, but safe to keep)
course_df.columns = [c.lower().strip().replace(' ', '_') for c in course_df.columns]

# Drop rows where 'name' or 'url' is missing (essential fields)
course_df = course_df.dropna(subset=['name', 'url'])

# Normalize 'name' and 'url'
course_df['name'] = course_df['name'].astype(str).str.strip()
course_df['url'] = course_df['url'].astype(str).str.strip()

# Normalize platform
course_df['platform'] = course_df['platform'].astype(str).str.strip()

# Fill any missing values just in case
course_df.fillna({
    'name': 'Untitled',
    'url': 'No link',
    'platform': 'Unknown'
}, inplace=True)

# Save to CSV
course_df[['name', 'url']].dropna().to_csv("/content/courses_final.csv", index=False)

print("‚úÖ Cleaned course dataset saved as /content/courses_final.csv")

import random
import pandas as pd

# Step 1: Load your existing course_df if not already
try:
    course_df = pd.read_csv("/content/courses_final.csv")
except Exception as e:
    print("‚ö†Ô∏è Error loading existing courses_final.csv:", e)
    course_df = pd.DataFrame(columns=["name", "url", "platform"])

# Step 2: Real curated course entries for common missing tech skills
manual_courses = [
    {"name": "Meta Front-End Developer - React", "url": "https://www.coursera.org/learn/meta-front-end-developer", "platform": "Coursera"},
    {"name": "Django for Beginners", "url": "https://www.udemy.com/course/django-for-beginners/", "platform": "Udemy"},
    {"name": "REST APIs with Flask and Python", "url": "https://www.udemy.com/course/rest-api-flask-and-python/", "platform": "Udemy"},
    {"name": "Master the Coding Interview: Data Structures", "url": "https://www.udemy.com/course/master-the-coding-interview-data-structures-algorithms/", "platform": "Udemy"},
    {"name": "Docker for DevOps", "url": "https://www.udemy.com/course/docker-mastery/", "platform": "Udemy"},
    {"name": "Version Control with Git", "url": "https://www.coursera.org/learn/introduction-git-github", "platform": "Coursera"},
    {"name": "Cloud Computing Basics (AWS)", "url": "https://www.coursera.org/learn/cloud-computing-basics", "platform": "Coursera"},
    {"name": "Full-Stack Web Development with React", "url": "https://www.coursera.org/specializations/full-stack-react", "platform": "Coursera"}
]

manual_df = pd.DataFrame(manual_courses)
manual_extra_courses = [
    {"name": "Cloud Computing Basics (Coursera - LearnQuest)", "url": "https://www.coursera.org/learn/cloud-computing-basics", "platform": "Coursera"},
    {"name": "Data Mining Specialization", "url": "https://www.coursera.org/specializations/data-mining", "platform": "Coursera"},
    {"name": "Data Visualization with Tableau", "url": "https://www.coursera.org/learn/visual-analytics", "platform": "Coursera"},
    {"name": "Hadoop Platform and Application Framework", "url": "https://www.coursera.org/learn/hadoop", "platform": "Coursera"},
    {"name": "NoSQL Database Systems", "url": "https://www.edx.org/course/nosql-database-systems", "platform": "edX"},
    {"name": "Big Data Analysis with Spark", "url": "https://www.edx.org/course/big-data-analysis-with-spark", "platform": "edX"},
    {"name": "ETL and Data Pipelines with Shell, Airflow, and Kafka", "url": "https://www.coursera.org/learn/etl-data-pipelines", "platform": "Coursera"},
    {"name": "Introduction to Statistical Analysis", "url": "https://www.edx.org/course/introduction-to-statistical-analysis", "platform": "edX"},
    {"name": "Data Modeling and Relational Database Design", "url": "https://www.coursera.org/learn/data-modeling", "platform": "Coursera"},
]

manual_df2 = pd.DataFrame(manual_extra_courses)
full_df = pd.concat([full_df, manual_df2], ignore_index=True)


# Step 3: Random dummy tech skills course generator
skills = [
    "cloud computing", "data analysis", "data mining", "data modeling", "data visualization",
    "etl", "hadoop", "nosql", "spark", "statistical analysis", "machine learning", "deep learning",
    "natural language processing", "python programming", "sql", "mongodb", "big data",
    "git", "docker", "kubernetes", "devops", "tensorflow", "scikit-learn", "pandas", "numpy",
    "r programming", "ai ethics", "llms", "prompt engineering", "azure", "aws", "gcp"
]

platforms = ["Coursera", "Udemy", "EdX", "DataCamp", "Pluralsight"]

# Step 4: Generate 100 random dummy courses
extra_courses = []
for i in range(100):
    skill = random.choice(skills)
    course = {
        "name": f"Mastering {skill.title()} - Level {random.randint(1, 3)}",
        "url": f"https://example.com/{skill.replace(' ', '-')}-course-{i+1}",
        "platform": random.choice(platforms)
    }
    extra_courses.append(course)

extra_df = pd.DataFrame(extra_courses)

# Step 5: Combine and deduplicate
full_df = pd.concat([course_df, manual_df, extra_df], ignore_index=True)
full_df.drop_duplicates(subset=['name', 'url'], inplace=True)

# Step 6: Save
full_df.to_csv("/content/courses_final.csv", index=False)
print(f"‚úÖ Final course list saved: {len(full_df)} total courses")

import re

for skill in missing:
    print(f"\nüîç Looking for courses on: {skill}")

    # Use regex for partial match (e.g., 'spark' matches 'Apache Spark', etc.)
    pattern = re.compile(rf'\b{re.escape(skill)}\b', re.IGNORECASE)

    matched_courses = course_df[course_df['name'].apply(lambda x: bool(pattern.search(str(x))))]

    if not matched_courses.empty:
        for _, row in matched_courses.head(2).iterrows():
            name = row.get('name', 'Untitled')
            link = row.get('url', 'No link')
            platform = row.get('platform', 'Unknown')
            print(f" - {skill.title()}: {name} ({platform}) ‚Üí {link}")
    else:
        print(f" - No course found for: {skill}")

course_df['skills'] = course_df['name'].str.lower()

course_df['name'].sample(10).tolist()

!pip install -q streamlit pyngrok

!ngrok config add-authtoken 2zb541a7beN0wZEpwT4lWfGZr6x_7Hi1J3T4eT5nQ85f4JrGa

!pip install PyMuPDF

from pyngrok import ngrok
import time
import os

# Kill existing
!pkill streamlit
ngrok.kill()

# Run Streamlit on port 8501
get_ipython().system_raw('streamlit run app.py --server.port 8501 > streamlit_log.txt 2>&1 &')

# Wait & expose with ngrok
time.sleep(8)
public_url = ngrok.connect("http://localhost:8501")
print(f"üåê App is live at: {public_url}")